{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff54ebf8-7962-45e2-be44-0590c071f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "import uuid\n",
    "\n",
    "import glob\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from eis.EISDataIO import eis_dataframe_from_csv\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52990bfb-ecae-48c4-a5b8-99b4f88cbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data\n",
    "\n",
    "# if you are on a windows machine un-comment the following line to get the path to training data\n",
    "# here = !echo %cd%\n",
    "\n",
    "# if you are on a mac/ unix machine un-comment the following line to get the path to training data\n",
    "here = !pwd\n",
    "\n",
    "train_data_path = path.join(path.dirname(here[0]), \"train_data.csv\")\n",
    "eis_data = eis_dataframe_from_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f4b0af6-f9f2-4462-a97f-318de75c6845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>Z</th>\n",
       "      <th>Circuit</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.1, 0.148398179, 0.220220195, 0.326802759, 0...</td>\n",
       "      <td>[(309.82561192-156.06088233j), (267.46983919-1...</td>\n",
       "      <td>L-R-RCPE-RCPE-RCPE</td>\n",
       "      <td>L1: 2.94e-08, R1: 4.51e+00, R2: 5.19e-02, CPE1...</td>\n",
       "      <td>/home/eis-user/images/L-R-RCPE-RCPE-RCPE/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0, 1.34339933, 1.80472177, 2.42446202, 3.25...</td>\n",
       "      <td>[(344.50700012-0.87321496j), (344.36191597-0.9...</td>\n",
       "      <td>RC-RC-RCPE-RCPE</td>\n",
       "      <td>R1: 2.08e+02, R2: 2.50e+01, R3: 9.57e+01, R4: ...</td>\n",
       "      <td>/home/eis-user/images/RC-RC-RCPE-RCPE/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 1.26360956, 1.59670912, 2.01761691, 2.54...</td>\n",
       "      <td>[(3080.15920083-80.84202473j), (3071.83539583-...</td>\n",
       "      <td>L-R-RCPE-RCPE-RCPE</td>\n",
       "      <td>L1: 3.35e-08, R1: 6.95e+01, R2: 7.49e+01, CPE1...</td>\n",
       "      <td>/home/eis-user/images/L-R-RCPE-RCPE-RCPE/2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[10.0, 13.4990445, 18.2224203, 24.5985264, 33....</td>\n",
       "      <td>[(930.93345951-0.0068507146j), (930.93327153-0...</td>\n",
       "      <td>L-R-RCPE</td>\n",
       "      <td>L1: 8.43e-07, R1: 9.06e+01, R2: 8.40e+02, CPE1...</td>\n",
       "      <td>/home/eis-user/images/L-R-RCPE/3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.01, 0.0148907532, 0.0221734532, 0.033017942...</td>\n",
       "      <td>[(405.07355219-0.0149508921j), (405.07110253-0...</td>\n",
       "      <td>RCPE-RCPE-RCPE</td>\n",
       "      <td>R1: 1.03e+01, R2: 6.71e-01, R3: 3.94e+02, CPE1...</td>\n",
       "      <td>/home/eis-user/images/RCPE-RCPE-RCPE/4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                freq  \\\n",
       "0  [0.1, 0.148398179, 0.220220195, 0.326802759, 0...   \n",
       "1  [1.0, 1.34339933, 1.80472177, 2.42446202, 3.25...   \n",
       "2  [1.0, 1.26360956, 1.59670912, 2.01761691, 2.54...   \n",
       "3  [10.0, 13.4990445, 18.2224203, 24.5985264, 33....   \n",
       "4  [0.01, 0.0148907532, 0.0221734532, 0.033017942...   \n",
       "\n",
       "                                                   Z             Circuit  \\\n",
       "0  [(309.82561192-156.06088233j), (267.46983919-1...  L-R-RCPE-RCPE-RCPE   \n",
       "1  [(344.50700012-0.87321496j), (344.36191597-0.9...     RC-RC-RCPE-RCPE   \n",
       "2  [(3080.15920083-80.84202473j), (3071.83539583-...  L-R-RCPE-RCPE-RCPE   \n",
       "3  [(930.93345951-0.0068507146j), (930.93327153-0...            L-R-RCPE   \n",
       "4  [(405.07355219-0.0149508921j), (405.07110253-0...      RCPE-RCPE-RCPE   \n",
       "\n",
       "                                          Parameters  \\\n",
       "0  L1: 2.94e-08, R1: 4.51e+00, R2: 5.19e-02, CPE1...   \n",
       "1  R1: 2.08e+02, R2: 2.50e+01, R3: 9.57e+01, R4: ...   \n",
       "2  L1: 3.35e-08, R1: 6.95e+01, R2: 7.49e+01, CPE1...   \n",
       "3  L1: 8.43e-07, R1: 9.06e+01, R2: 8.40e+02, CPE1...   \n",
       "4  R1: 1.03e+01, R2: 6.71e-01, R3: 3.94e+02, CPE1...   \n",
       "\n",
       "                                       image_name  \n",
       "0  /home/eis-user/images/L-R-RCPE-RCPE-RCPE/0.jpg  \n",
       "1     /home/eis-user/images/RC-RC-RCPE-RCPE/1.jpg  \n",
       "2  /home/eis-user/images/L-R-RCPE-RCPE-RCPE/2.jpg  \n",
       "3            /home/eis-user/images/L-R-RCPE/3.jpg  \n",
       "4      /home/eis-user/images/RCPE-RCPE-RCPE/4.jpg  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add image path\n",
    "image_folders_path = path.join(path.dirname(here[0]),\"images/\")\n",
    "\n",
    "eis_data[\"image_name\"] = image_folders_path + eis_data[\"Circuit\"] + \"/\" + eis_data.index.astype(str)+\".jpg\"\n",
    "eis_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f23f82d2-030e-4c03-8708-f1254945e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "def create_cnn_model(num_classes: int,initial_lr:float = 0.01):\n",
    "    inputs = layers.Input(shape=(224, 224, 3))\n",
    "    base_model = keras.applications.MobileNetV2(input_shape=(224,224,3),input_tensor=inputs,weights=\"imagenet\",include_top=False)\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense(32,activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(16,activation=\"relu\")(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"MobileNetV2\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4aec0147-2d5c-41c3-97d8-2a7c4928c1ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MobileNetV2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1280)        5120        ['avg_pool[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           40992       ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32)          128         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           528         ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " top_dropout (Dropout)          (None, 16)           0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " pred (Dense)                   (None, 9)            153         ['top_dropout[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,304,905\n",
      "Trainable params: 44,297\n",
      "Non-trainable params: 2,260,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_cnn_model(num_classes=9,initial_lr=0.01)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba724e88-460d-48a2-ad37-8a93eada68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5224 validated image filenames belonging to 9 classes.\n",
      "Found 2238 validated image filenames belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data generator\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255.,validation_split=0.3)\n",
    "\n",
    "\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(dataframe = eis_data,\n",
    "                                        x_col=\"image_name\",\n",
    "                                        y_col=\"Circuit\",\n",
    "                                        subset=\"training\",\n",
    "                                        batch_size=32,\n",
    "                                        shuffle=True,\n",
    "                                        seed=42,\n",
    "                                        target_size=(224,224),\n",
    "                                        class_mode=\"categorical\"\n",
    "                                        )\n",
    "val_gen = datagen.flow_from_dataframe(dataframe = eis_data,\n",
    "                                        x_col=\"image_name\",\n",
    "                                        y_col=\"Circuit\",\n",
    "                                        subset=\"validation\",\n",
    "                                        batch_size=32,\n",
    "                                        shuffle=False,\n",
    "                                        seed=42,\n",
    "                                        target_size=(224,224),\n",
    "                                        class_mode=\"categorical\"\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3171417a-3e11-4ea6-8904-97f250aa79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7447/2162743453.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  h = model.fit_generator(generator = train_gen,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 73s 450ms/step - loss: 1.7073 - accuracy: 0.3230 - val_loss: 1.7338 - val_accuracy: 0.3265\n",
      "Epoch 2/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.6295 - accuracy: 0.3482 - val_loss: 1.6103 - val_accuracy: 0.3401\n",
      "Epoch 3/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 1.5750 - accuracy: 0.3787 - val_loss: 1.5686 - val_accuracy: 0.3705\n",
      "Epoch 4/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 1.5482 - accuracy: 0.3746 - val_loss: 1.5875 - val_accuracy: 0.3564\n",
      "Epoch 5/1000\n",
      "163/163 [==============================] - 70s 430ms/step - loss: 1.5200 - accuracy: 0.3858 - val_loss: 1.5853 - val_accuracy: 0.3773\n",
      "Epoch 6/1000\n",
      "163/163 [==============================] - 70s 429ms/step - loss: 1.4787 - accuracy: 0.3960 - val_loss: 1.6352 - val_accuracy: 0.3623\n",
      "Epoch 7/1000\n",
      "163/163 [==============================] - 69s 424ms/step - loss: 1.4800 - accuracy: 0.3995 - val_loss: 1.5558 - val_accuracy: 0.3800\n",
      "Epoch 8/1000\n",
      "163/163 [==============================] - 70s 427ms/step - loss: 1.4502 - accuracy: 0.4178 - val_loss: 1.5760 - val_accuracy: 0.3818\n",
      "Epoch 9/1000\n",
      "163/163 [==============================] - 70s 433ms/step - loss: 1.4345 - accuracy: 0.4218 - val_loss: 1.5875 - val_accuracy: 0.3854\n",
      "Epoch 10/1000\n",
      "163/163 [==============================] - 69s 423ms/step - loss: 1.3997 - accuracy: 0.4374 - val_loss: 1.6026 - val_accuracy: 0.3822\n",
      "Epoch 11/1000\n",
      "163/163 [==============================] - 69s 425ms/step - loss: 1.4053 - accuracy: 0.4212 - val_loss: 1.6360 - val_accuracy: 0.3673\n",
      "Epoch 12/1000\n",
      "163/163 [==============================] - 70s 428ms/step - loss: 1.3926 - accuracy: 0.4243 - val_loss: 1.6137 - val_accuracy: 0.3696\n",
      "Epoch 13/1000\n",
      "163/163 [==============================] - 69s 425ms/step - loss: 1.3828 - accuracy: 0.4366 - val_loss: 1.6226 - val_accuracy: 0.3764\n",
      "Epoch 14/1000\n",
      "163/163 [==============================] - 70s 429ms/step - loss: 1.3503 - accuracy: 0.4486 - val_loss: 1.6316 - val_accuracy: 0.3813\n",
      "Epoch 15/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 1.3669 - accuracy: 0.4526 - val_loss: 1.6250 - val_accuracy: 0.3904\n",
      "Epoch 16/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 1.3358 - accuracy: 0.4522 - val_loss: 1.6501 - val_accuracy: 0.3755\n",
      "Epoch 17/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 1.3265 - accuracy: 0.4613 - val_loss: 1.6757 - val_accuracy: 0.3619\n",
      "Epoch 18/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 1.3377 - accuracy: 0.4599 - val_loss: 1.7028 - val_accuracy: 0.3773\n",
      "Epoch 19/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.3008 - accuracy: 0.4657 - val_loss: 1.7581 - val_accuracy: 0.3573\n",
      "Epoch 20/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.2978 - accuracy: 0.4769 - val_loss: 1.7161 - val_accuracy: 0.3650\n",
      "Epoch 21/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 1.2943 - accuracy: 0.4684 - val_loss: 1.6512 - val_accuracy: 0.3913\n",
      "Epoch 22/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.2749 - accuracy: 0.4829 - val_loss: 1.7241 - val_accuracy: 0.3890\n",
      "Epoch 23/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 1.2745 - accuracy: 0.4804 - val_loss: 1.8387 - val_accuracy: 0.3546\n",
      "Epoch 24/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 1.3076 - accuracy: 0.4761 - val_loss: 1.7277 - val_accuracy: 0.3623\n",
      "Epoch 25/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 1.2900 - accuracy: 0.4792 - val_loss: 1.8410 - val_accuracy: 0.3723\n",
      "Epoch 26/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.2597 - accuracy: 0.4829 - val_loss: 1.7913 - val_accuracy: 0.3922\n",
      "Epoch 27/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.2466 - accuracy: 0.4890 - val_loss: 1.7775 - val_accuracy: 0.3791\n",
      "Epoch 28/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.2620 - accuracy: 0.4900 - val_loss: 1.7387 - val_accuracy: 0.3827\n",
      "Epoch 29/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.2539 - accuracy: 0.4909 - val_loss: 1.7703 - val_accuracy: 0.3564\n",
      "Epoch 30/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.2173 - accuracy: 0.5042 - val_loss: 1.7876 - val_accuracy: 0.3732\n",
      "Epoch 31/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.2184 - accuracy: 0.5081 - val_loss: 1.8216 - val_accuracy: 0.3673\n",
      "Epoch 32/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.2295 - accuracy: 0.5037 - val_loss: 1.7718 - val_accuracy: 0.3696\n",
      "Epoch 33/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.2099 - accuracy: 0.5050 - val_loss: 2.0304 - val_accuracy: 0.3718\n",
      "Epoch 34/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.1972 - accuracy: 0.5154 - val_loss: 1.8330 - val_accuracy: 0.3927\n",
      "Epoch 35/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 1.1864 - accuracy: 0.5162 - val_loss: 1.8087 - val_accuracy: 0.3741\n",
      "Epoch 36/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.1825 - accuracy: 0.5206 - val_loss: 1.7807 - val_accuracy: 0.3569\n",
      "Epoch 37/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 1.1799 - accuracy: 0.5193 - val_loss: 1.8127 - val_accuracy: 0.3759\n",
      "Epoch 38/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 1.1790 - accuracy: 0.5223 - val_loss: 1.9345 - val_accuracy: 0.3727\n",
      "Epoch 39/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.1750 - accuracy: 0.5239 - val_loss: 1.8726 - val_accuracy: 0.3759\n",
      "Epoch 40/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 1.1755 - accuracy: 0.5247 - val_loss: 1.8942 - val_accuracy: 0.3759\n",
      "Epoch 41/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 1.1550 - accuracy: 0.5245 - val_loss: 1.9482 - val_accuracy: 0.3764\n",
      "Epoch 42/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 1.1594 - accuracy: 0.5221 - val_loss: 1.9103 - val_accuracy: 0.3714\n",
      "Epoch 43/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 1.1528 - accuracy: 0.5299 - val_loss: 1.8910 - val_accuracy: 0.3727\n",
      "Epoch 44/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.1249 - accuracy: 0.5387 - val_loss: 2.0657 - val_accuracy: 0.3709\n",
      "Epoch 45/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 1.1439 - accuracy: 0.5397 - val_loss: 1.8780 - val_accuracy: 0.3718\n",
      "Epoch 46/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 1.1204 - accuracy: 0.5476 - val_loss: 2.0459 - val_accuracy: 0.3678\n",
      "Epoch 47/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.1418 - accuracy: 0.5408 - val_loss: 1.9385 - val_accuracy: 0.3668\n",
      "Epoch 48/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 1.1506 - accuracy: 0.5354 - val_loss: 1.9204 - val_accuracy: 0.3727\n",
      "Epoch 49/1000\n",
      "163/163 [==============================] - 87s 536ms/step - loss: 1.1203 - accuracy: 0.5445 - val_loss: 1.9513 - val_accuracy: 0.3650\n",
      "Epoch 50/1000\n",
      "163/163 [==============================] - 77s 474ms/step - loss: 1.1292 - accuracy: 0.5399 - val_loss: 2.1469 - val_accuracy: 0.3714\n",
      "Epoch 51/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.1266 - accuracy: 0.5393 - val_loss: 1.9449 - val_accuracy: 0.3659\n",
      "Epoch 52/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.1163 - accuracy: 0.5439 - val_loss: 1.9741 - val_accuracy: 0.3773\n",
      "Epoch 53/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 1.1120 - accuracy: 0.5510 - val_loss: 2.0026 - val_accuracy: 0.3659\n",
      "Epoch 54/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.1039 - accuracy: 0.5582 - val_loss: 2.0504 - val_accuracy: 0.3700\n",
      "Epoch 55/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 1.1200 - accuracy: 0.5499 - val_loss: 2.0345 - val_accuracy: 0.3388\n",
      "Epoch 56/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.1136 - accuracy: 0.5466 - val_loss: 2.1000 - val_accuracy: 0.3655\n",
      "Epoch 57/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.1158 - accuracy: 0.5462 - val_loss: 1.9442 - val_accuracy: 0.3773\n",
      "Epoch 58/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 1.0880 - accuracy: 0.5586 - val_loss: 2.0043 - val_accuracy: 0.3601\n",
      "Epoch 59/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 1.1088 - accuracy: 0.5584 - val_loss: 1.9241 - val_accuracy: 0.3727\n",
      "Epoch 60/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.1001 - accuracy: 0.5466 - val_loss: 2.0673 - val_accuracy: 0.3447\n",
      "Epoch 61/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 1.0810 - accuracy: 0.5551 - val_loss: 2.1144 - val_accuracy: 0.3551\n",
      "Epoch 62/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 1.1072 - accuracy: 0.5607 - val_loss: 2.1065 - val_accuracy: 0.3673\n",
      "Epoch 63/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0856 - accuracy: 0.5599 - val_loss: 2.0285 - val_accuracy: 0.3741\n",
      "Epoch 64/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0895 - accuracy: 0.5586 - val_loss: 2.1018 - val_accuracy: 0.3632\n",
      "Epoch 65/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0712 - accuracy: 0.5666 - val_loss: 1.9811 - val_accuracy: 0.3691\n",
      "Epoch 66/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0605 - accuracy: 0.5788 - val_loss: 2.2763 - val_accuracy: 0.3673\n",
      "Epoch 67/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 1.0855 - accuracy: 0.5605 - val_loss: 2.1791 - val_accuracy: 0.3601\n",
      "Epoch 68/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 1.0807 - accuracy: 0.5599 - val_loss: 2.1134 - val_accuracy: 0.3560\n",
      "Epoch 69/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 1.0632 - accuracy: 0.5732 - val_loss: 2.1540 - val_accuracy: 0.3591\n",
      "Epoch 70/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0548 - accuracy: 0.5763 - val_loss: 2.1515 - val_accuracy: 0.3655\n",
      "Epoch 71/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 1.0547 - accuracy: 0.5809 - val_loss: 2.1626 - val_accuracy: 0.3578\n",
      "Epoch 72/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.0558 - accuracy: 0.5680 - val_loss: 2.1846 - val_accuracy: 0.3664\n",
      "Epoch 73/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0571 - accuracy: 0.5743 - val_loss: 2.2474 - val_accuracy: 0.3619\n",
      "Epoch 74/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 1.0662 - accuracy: 0.5682 - val_loss: 2.2307 - val_accuracy: 0.3664\n",
      "Epoch 75/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 1.0403 - accuracy: 0.5828 - val_loss: 2.3451 - val_accuracy: 0.3582\n",
      "Epoch 76/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0359 - accuracy: 0.5795 - val_loss: 2.2954 - val_accuracy: 0.3632\n",
      "Epoch 77/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 1.0204 - accuracy: 0.5776 - val_loss: 2.3220 - val_accuracy: 0.3759\n",
      "Epoch 78/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.0336 - accuracy: 0.5780 - val_loss: 2.1579 - val_accuracy: 0.3777\n",
      "Epoch 79/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 1.0108 - accuracy: 0.5907 - val_loss: 2.3082 - val_accuracy: 0.3682\n",
      "Epoch 80/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.0178 - accuracy: 0.5855 - val_loss: 2.2170 - val_accuracy: 0.3718\n",
      "Epoch 81/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 1.0219 - accuracy: 0.5830 - val_loss: 2.3607 - val_accuracy: 0.3791\n",
      "Epoch 82/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.0378 - accuracy: 0.5844 - val_loss: 2.3074 - val_accuracy: 0.3605\n",
      "Epoch 83/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0009 - accuracy: 0.5913 - val_loss: 2.2953 - val_accuracy: 0.3533\n",
      "Epoch 84/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0161 - accuracy: 0.5955 - val_loss: 2.1306 - val_accuracy: 0.3628\n",
      "Epoch 85/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 1.0147 - accuracy: 0.5863 - val_loss: 2.1235 - val_accuracy: 0.3605\n",
      "Epoch 86/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 1.0507 - accuracy: 0.5820 - val_loss: 2.2154 - val_accuracy: 0.3596\n",
      "Epoch 87/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 1.0275 - accuracy: 0.5884 - val_loss: 2.3530 - val_accuracy: 0.3818\n",
      "Epoch 88/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0093 - accuracy: 0.5936 - val_loss: 2.2787 - val_accuracy: 0.3723\n",
      "Epoch 89/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.9979 - accuracy: 0.5994 - val_loss: 2.1593 - val_accuracy: 0.3637\n",
      "Epoch 90/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 1.0003 - accuracy: 0.5957 - val_loss: 2.1498 - val_accuracy: 0.3696\n",
      "Epoch 91/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9791 - accuracy: 0.6029 - val_loss: 2.4615 - val_accuracy: 0.3773\n",
      "Epoch 92/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0135 - accuracy: 0.5874 - val_loss: 2.3762 - val_accuracy: 0.3709\n",
      "Epoch 93/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.9893 - accuracy: 0.6025 - val_loss: 2.3744 - val_accuracy: 0.3569\n",
      "Epoch 94/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 1.0111 - accuracy: 0.5921 - val_loss: 2.4613 - val_accuracy: 0.3619\n",
      "Epoch 95/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.9950 - accuracy: 0.5940 - val_loss: 2.3608 - val_accuracy: 0.3637\n",
      "Epoch 96/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 1.0002 - accuracy: 0.5977 - val_loss: 2.3118 - val_accuracy: 0.3659\n",
      "Epoch 97/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9925 - accuracy: 0.6003 - val_loss: 2.3247 - val_accuracy: 0.3700\n",
      "Epoch 98/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 1.0146 - accuracy: 0.6029 - val_loss: 2.3531 - val_accuracy: 0.3791\n",
      "Epoch 99/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.9744 - accuracy: 0.6092 - val_loss: 2.4349 - val_accuracy: 0.3605\n",
      "Epoch 100/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9825 - accuracy: 0.6007 - val_loss: 2.5586 - val_accuracy: 0.3623\n",
      "Epoch 101/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.9762 - accuracy: 0.6167 - val_loss: 2.3460 - val_accuracy: 0.3537\n",
      "Epoch 102/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9908 - accuracy: 0.6075 - val_loss: 2.3446 - val_accuracy: 0.3727\n",
      "Epoch 103/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.9884 - accuracy: 0.6109 - val_loss: 2.3498 - val_accuracy: 0.3524\n",
      "Epoch 104/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.9516 - accuracy: 0.6081 - val_loss: 2.3062 - val_accuracy: 0.3750\n",
      "Epoch 105/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9905 - accuracy: 0.6098 - val_loss: 2.3894 - val_accuracy: 0.3578\n",
      "Epoch 106/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.9759 - accuracy: 0.6038 - val_loss: 2.3334 - val_accuracy: 0.3705\n",
      "Epoch 107/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.9713 - accuracy: 0.6102 - val_loss: 2.3672 - val_accuracy: 0.3696\n",
      "Epoch 108/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.9859 - accuracy: 0.6088 - val_loss: 2.6640 - val_accuracy: 0.3682\n",
      "Epoch 109/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.9562 - accuracy: 0.6177 - val_loss: 2.6549 - val_accuracy: 0.3700\n",
      "Epoch 110/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.9512 - accuracy: 0.6211 - val_loss: 2.5165 - val_accuracy: 0.3569\n",
      "Epoch 111/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.9315 - accuracy: 0.6262 - val_loss: 2.6510 - val_accuracy: 0.3537\n",
      "Epoch 112/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.9508 - accuracy: 0.6169 - val_loss: 2.5465 - val_accuracy: 0.3524\n",
      "Epoch 113/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.9667 - accuracy: 0.6161 - val_loss: 2.5037 - val_accuracy: 0.3524\n",
      "Epoch 114/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.9525 - accuracy: 0.6221 - val_loss: 2.6876 - val_accuracy: 0.3546\n",
      "Epoch 115/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.9641 - accuracy: 0.6198 - val_loss: 2.5191 - val_accuracy: 0.3682\n",
      "Epoch 116/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.9512 - accuracy: 0.6221 - val_loss: 2.6033 - val_accuracy: 0.3573\n",
      "Epoch 117/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.9331 - accuracy: 0.6244 - val_loss: 2.6464 - val_accuracy: 0.3519\n",
      "Epoch 118/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.9343 - accuracy: 0.6306 - val_loss: 2.5674 - val_accuracy: 0.3469\n",
      "Epoch 119/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9381 - accuracy: 0.6258 - val_loss: 2.5045 - val_accuracy: 0.3673\n",
      "Epoch 120/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9482 - accuracy: 0.6188 - val_loss: 2.6576 - val_accuracy: 0.3560\n",
      "Epoch 121/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.9347 - accuracy: 0.6333 - val_loss: 2.5106 - val_accuracy: 0.3569\n",
      "Epoch 122/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.9182 - accuracy: 0.6315 - val_loss: 2.7941 - val_accuracy: 0.3628\n",
      "Epoch 123/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.9506 - accuracy: 0.6254 - val_loss: 2.4971 - val_accuracy: 0.3587\n",
      "Epoch 124/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.9248 - accuracy: 0.6223 - val_loss: 2.5378 - val_accuracy: 0.3569\n",
      "Epoch 125/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.9145 - accuracy: 0.6414 - val_loss: 2.5889 - val_accuracy: 0.3533\n",
      "Epoch 126/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.9590 - accuracy: 0.6254 - val_loss: 2.4437 - val_accuracy: 0.3673\n",
      "Epoch 127/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.9075 - accuracy: 0.6367 - val_loss: 2.5827 - val_accuracy: 0.3474\n",
      "Epoch 128/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.9262 - accuracy: 0.6315 - val_loss: 2.5355 - val_accuracy: 0.3700\n",
      "Epoch 129/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.9158 - accuracy: 0.6371 - val_loss: 2.5394 - val_accuracy: 0.3664\n",
      "Epoch 130/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.9239 - accuracy: 0.6321 - val_loss: 2.4508 - val_accuracy: 0.3655\n",
      "Epoch 131/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.9205 - accuracy: 0.6360 - val_loss: 2.5402 - val_accuracy: 0.3682\n",
      "Epoch 132/1000\n",
      "163/163 [==============================] - 76s 468ms/step - loss: 0.9168 - accuracy: 0.6462 - val_loss: 2.4982 - val_accuracy: 0.3551\n",
      "Epoch 133/1000\n",
      "163/163 [==============================] - 75s 462ms/step - loss: 0.9073 - accuracy: 0.6325 - val_loss: 2.5605 - val_accuracy: 0.3619\n",
      "Epoch 134/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.9204 - accuracy: 0.6373 - val_loss: 2.5625 - val_accuracy: 0.3596\n",
      "Epoch 135/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.9247 - accuracy: 0.6408 - val_loss: 2.6884 - val_accuracy: 0.3587\n",
      "Epoch 136/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.9088 - accuracy: 0.6414 - val_loss: 2.6250 - val_accuracy: 0.3478\n",
      "Epoch 137/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.8841 - accuracy: 0.6448 - val_loss: 2.6418 - val_accuracy: 0.3551\n",
      "Epoch 138/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8976 - accuracy: 0.6427 - val_loss: 2.7987 - val_accuracy: 0.3696\n",
      "Epoch 139/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.8910 - accuracy: 0.6437 - val_loss: 2.7726 - val_accuracy: 0.3496\n",
      "Epoch 140/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.9379 - accuracy: 0.6321 - val_loss: 2.6026 - val_accuracy: 0.3623\n",
      "Epoch 141/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8824 - accuracy: 0.6473 - val_loss: 2.7653 - val_accuracy: 0.3623\n",
      "Epoch 142/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8919 - accuracy: 0.6450 - val_loss: 2.6090 - val_accuracy: 0.3510\n",
      "Epoch 143/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.9127 - accuracy: 0.6423 - val_loss: 2.6361 - val_accuracy: 0.3614\n",
      "Epoch 144/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.8700 - accuracy: 0.6568 - val_loss: 2.7004 - val_accuracy: 0.3560\n",
      "Epoch 145/1000\n",
      "163/163 [==============================] - 92s 564ms/step - loss: 0.9002 - accuracy: 0.6477 - val_loss: 2.7696 - val_accuracy: 0.3573\n",
      "Epoch 146/1000\n",
      "163/163 [==============================] - 79s 488ms/step - loss: 0.9072 - accuracy: 0.6427 - val_loss: 2.6465 - val_accuracy: 0.3673\n",
      "Epoch 147/1000\n",
      "163/163 [==============================] - 70s 430ms/step - loss: 0.8941 - accuracy: 0.6514 - val_loss: 2.5556 - val_accuracy: 0.3623\n",
      "Epoch 148/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.8729 - accuracy: 0.6547 - val_loss: 2.7043 - val_accuracy: 0.3582\n",
      "Epoch 149/1000\n",
      "163/163 [==============================] - 70s 430ms/step - loss: 0.8953 - accuracy: 0.6477 - val_loss: 2.6112 - val_accuracy: 0.3483\n",
      "Epoch 150/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.9174 - accuracy: 0.6379 - val_loss: 2.7709 - val_accuracy: 0.3773\n",
      "Epoch 151/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.9010 - accuracy: 0.6489 - val_loss: 2.5371 - val_accuracy: 0.3605\n",
      "Epoch 152/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.8958 - accuracy: 0.6479 - val_loss: 2.5953 - val_accuracy: 0.3587\n",
      "Epoch 153/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.8768 - accuracy: 0.6577 - val_loss: 2.8003 - val_accuracy: 0.3546\n",
      "Epoch 154/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.8712 - accuracy: 0.6616 - val_loss: 2.6083 - val_accuracy: 0.3650\n",
      "Epoch 155/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.8804 - accuracy: 0.6610 - val_loss: 2.7762 - val_accuracy: 0.3514\n",
      "Epoch 156/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.8856 - accuracy: 0.6471 - val_loss: 2.8676 - val_accuracy: 0.3438\n",
      "Epoch 157/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.9062 - accuracy: 0.6445 - val_loss: 2.5085 - val_accuracy: 0.3700\n",
      "Epoch 158/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8832 - accuracy: 0.6541 - val_loss: 2.8189 - val_accuracy: 0.3546\n",
      "Epoch 159/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.8755 - accuracy: 0.6543 - val_loss: 2.8121 - val_accuracy: 0.3456\n",
      "Epoch 160/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8759 - accuracy: 0.6579 - val_loss: 2.6225 - val_accuracy: 0.3682\n",
      "Epoch 161/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8675 - accuracy: 0.6618 - val_loss: 2.6967 - val_accuracy: 0.3610\n",
      "Epoch 162/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8748 - accuracy: 0.6577 - val_loss: 2.7464 - val_accuracy: 0.3496\n",
      "Epoch 163/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8640 - accuracy: 0.6651 - val_loss: 2.7600 - val_accuracy: 0.3619\n",
      "Epoch 164/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8538 - accuracy: 0.6626 - val_loss: 2.7789 - val_accuracy: 0.3678\n",
      "Epoch 165/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8648 - accuracy: 0.6568 - val_loss: 2.8196 - val_accuracy: 0.3460\n",
      "Epoch 166/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.8584 - accuracy: 0.6612 - val_loss: 2.6609 - val_accuracy: 0.3673\n",
      "Epoch 167/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8711 - accuracy: 0.6591 - val_loss: 2.6107 - val_accuracy: 0.3428\n",
      "Epoch 168/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8744 - accuracy: 0.6554 - val_loss: 2.7930 - val_accuracy: 0.3582\n",
      "Epoch 169/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8592 - accuracy: 0.6668 - val_loss: 2.6475 - val_accuracy: 0.3673\n",
      "Epoch 170/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8632 - accuracy: 0.6651 - val_loss: 2.6725 - val_accuracy: 0.3623\n",
      "Epoch 171/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8706 - accuracy: 0.6662 - val_loss: 2.9107 - val_accuracy: 0.3524\n",
      "Epoch 172/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8717 - accuracy: 0.6604 - val_loss: 2.6862 - val_accuracy: 0.3578\n",
      "Epoch 173/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.8772 - accuracy: 0.6626 - val_loss: 2.5501 - val_accuracy: 0.3709\n",
      "Epoch 174/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8477 - accuracy: 0.6712 - val_loss: 2.8280 - val_accuracy: 0.3524\n",
      "Epoch 175/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.8351 - accuracy: 0.6722 - val_loss: 2.6836 - val_accuracy: 0.3528\n",
      "Epoch 176/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8621 - accuracy: 0.6628 - val_loss: 2.7023 - val_accuracy: 0.3578\n",
      "Epoch 177/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.8500 - accuracy: 0.6680 - val_loss: 2.6987 - val_accuracy: 0.3533\n",
      "Epoch 178/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.8737 - accuracy: 0.6537 - val_loss: 2.8846 - val_accuracy: 0.3632\n",
      "Epoch 179/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.8536 - accuracy: 0.6720 - val_loss: 2.8776 - val_accuracy: 0.3447\n",
      "Epoch 180/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8547 - accuracy: 0.6683 - val_loss: 2.7339 - val_accuracy: 0.3537\n",
      "Epoch 181/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8461 - accuracy: 0.6681 - val_loss: 2.7781 - val_accuracy: 0.3564\n",
      "Epoch 182/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8267 - accuracy: 0.6760 - val_loss: 2.7742 - val_accuracy: 0.3564\n",
      "Epoch 183/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8684 - accuracy: 0.6608 - val_loss: 2.6381 - val_accuracy: 0.3578\n",
      "Epoch 184/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8387 - accuracy: 0.6774 - val_loss: 2.8650 - val_accuracy: 0.3614\n",
      "Epoch 185/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8139 - accuracy: 0.6814 - val_loss: 2.8419 - val_accuracy: 0.3397\n",
      "Epoch 186/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.8754 - accuracy: 0.6603 - val_loss: 2.7658 - val_accuracy: 0.3442\n",
      "Epoch 187/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8599 - accuracy: 0.6708 - val_loss: 2.6998 - val_accuracy: 0.3514\n",
      "Epoch 188/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8800 - accuracy: 0.6572 - val_loss: 2.6511 - val_accuracy: 0.3533\n",
      "Epoch 189/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.8397 - accuracy: 0.6718 - val_loss: 2.8158 - val_accuracy: 0.3641\n",
      "Epoch 190/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8529 - accuracy: 0.6691 - val_loss: 2.6899 - val_accuracy: 0.3564\n",
      "Epoch 191/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.8513 - accuracy: 0.6653 - val_loss: 2.8668 - val_accuracy: 0.3555\n",
      "Epoch 192/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.8435 - accuracy: 0.6693 - val_loss: 2.8674 - val_accuracy: 0.3582\n",
      "Epoch 193/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.8545 - accuracy: 0.6724 - val_loss: 2.8059 - val_accuracy: 0.3646\n",
      "Epoch 194/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8675 - accuracy: 0.6668 - val_loss: 2.6637 - val_accuracy: 0.3587\n",
      "Epoch 195/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.8587 - accuracy: 0.6703 - val_loss: 2.7805 - val_accuracy: 0.3646\n",
      "Epoch 196/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8513 - accuracy: 0.6691 - val_loss: 2.6865 - val_accuracy: 0.3637\n",
      "Epoch 197/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8405 - accuracy: 0.6749 - val_loss: 2.8483 - val_accuracy: 0.3601\n",
      "Epoch 198/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8744 - accuracy: 0.6610 - val_loss: 2.6371 - val_accuracy: 0.3438\n",
      "Epoch 199/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8400 - accuracy: 0.6749 - val_loss: 2.6482 - val_accuracy: 0.3605\n",
      "Epoch 200/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8400 - accuracy: 0.6693 - val_loss: 2.6535 - val_accuracy: 0.3492\n",
      "Epoch 201/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.8341 - accuracy: 0.6724 - val_loss: 2.8783 - val_accuracy: 0.3795\n",
      "Epoch 202/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8649 - accuracy: 0.6597 - val_loss: 2.8557 - val_accuracy: 0.3664\n",
      "Epoch 203/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.8371 - accuracy: 0.6735 - val_loss: 2.6098 - val_accuracy: 0.3723\n",
      "Epoch 204/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8237 - accuracy: 0.6849 - val_loss: 2.8843 - val_accuracy: 0.3496\n",
      "Epoch 205/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.8226 - accuracy: 0.6724 - val_loss: 2.9815 - val_accuracy: 0.3424\n",
      "Epoch 206/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8389 - accuracy: 0.6768 - val_loss: 2.9639 - val_accuracy: 0.3591\n",
      "Epoch 207/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8392 - accuracy: 0.6733 - val_loss: 2.9277 - val_accuracy: 0.3542\n",
      "Epoch 208/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7956 - accuracy: 0.6853 - val_loss: 2.9394 - val_accuracy: 0.3741\n",
      "Epoch 209/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8464 - accuracy: 0.6681 - val_loss: 2.9926 - val_accuracy: 0.3587\n",
      "Epoch 210/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.8442 - accuracy: 0.6784 - val_loss: 2.7445 - val_accuracy: 0.3623\n",
      "Epoch 211/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8270 - accuracy: 0.6753 - val_loss: 3.0697 - val_accuracy: 0.3569\n",
      "Epoch 212/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7992 - accuracy: 0.6909 - val_loss: 2.9827 - val_accuracy: 0.3582\n",
      "Epoch 213/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.8234 - accuracy: 0.6861 - val_loss: 2.8610 - val_accuracy: 0.3528\n",
      "Epoch 214/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8191 - accuracy: 0.6772 - val_loss: 2.9484 - val_accuracy: 0.3560\n",
      "Epoch 215/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8119 - accuracy: 0.6818 - val_loss: 2.9772 - val_accuracy: 0.3465\n",
      "Epoch 216/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8184 - accuracy: 0.6901 - val_loss: 3.1209 - val_accuracy: 0.3745\n",
      "Epoch 217/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8215 - accuracy: 0.6807 - val_loss: 2.8418 - val_accuracy: 0.3614\n",
      "Epoch 218/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.8016 - accuracy: 0.6884 - val_loss: 2.9289 - val_accuracy: 0.3492\n",
      "Epoch 219/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8152 - accuracy: 0.6886 - val_loss: 2.8215 - val_accuracy: 0.3505\n",
      "Epoch 220/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8151 - accuracy: 0.6878 - val_loss: 2.9448 - val_accuracy: 0.3487\n",
      "Epoch 221/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7884 - accuracy: 0.6947 - val_loss: 3.1294 - val_accuracy: 0.3492\n",
      "Epoch 222/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.8493 - accuracy: 0.6776 - val_loss: 2.8495 - val_accuracy: 0.3501\n",
      "Epoch 223/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.8123 - accuracy: 0.6874 - val_loss: 3.0034 - val_accuracy: 0.3492\n",
      "Epoch 224/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7698 - accuracy: 0.6990 - val_loss: 3.2601 - val_accuracy: 0.3501\n",
      "Epoch 225/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.8093 - accuracy: 0.6903 - val_loss: 2.7509 - val_accuracy: 0.3533\n",
      "Epoch 226/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.8198 - accuracy: 0.6888 - val_loss: 3.0326 - val_accuracy: 0.3619\n",
      "Epoch 227/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8055 - accuracy: 0.6882 - val_loss: 2.8241 - val_accuracy: 0.3678\n",
      "Epoch 228/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8007 - accuracy: 0.6932 - val_loss: 2.8972 - val_accuracy: 0.3632\n",
      "Epoch 229/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8014 - accuracy: 0.6968 - val_loss: 2.6856 - val_accuracy: 0.3469\n",
      "Epoch 230/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8076 - accuracy: 0.6913 - val_loss: 3.0282 - val_accuracy: 0.3578\n",
      "Epoch 231/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8270 - accuracy: 0.6843 - val_loss: 2.8436 - val_accuracy: 0.3755\n",
      "Epoch 232/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.8186 - accuracy: 0.6914 - val_loss: 2.8125 - val_accuracy: 0.3655\n",
      "Epoch 233/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.8187 - accuracy: 0.6857 - val_loss: 2.8927 - val_accuracy: 0.3614\n",
      "Epoch 234/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8065 - accuracy: 0.6905 - val_loss: 2.8453 - val_accuracy: 0.3564\n",
      "Epoch 235/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7929 - accuracy: 0.6936 - val_loss: 2.9429 - val_accuracy: 0.3564\n",
      "Epoch 236/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.8258 - accuracy: 0.6870 - val_loss: 2.9980 - val_accuracy: 0.3456\n",
      "Epoch 237/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.8047 - accuracy: 0.6893 - val_loss: 2.8766 - val_accuracy: 0.3456\n",
      "Epoch 238/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.7858 - accuracy: 0.6940 - val_loss: 2.8962 - val_accuracy: 0.3487\n",
      "Epoch 239/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.8623 - accuracy: 0.6770 - val_loss: 2.8481 - val_accuracy: 0.3524\n",
      "Epoch 240/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.8096 - accuracy: 0.6843 - val_loss: 2.8901 - val_accuracy: 0.3442\n",
      "Epoch 241/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.8131 - accuracy: 0.6965 - val_loss: 2.8503 - val_accuracy: 0.3496\n",
      "Epoch 242/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7979 - accuracy: 0.6974 - val_loss: 2.9831 - val_accuracy: 0.3533\n",
      "Epoch 243/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7781 - accuracy: 0.6963 - val_loss: 2.9626 - val_accuracy: 0.3596\n",
      "Epoch 244/1000\n",
      "163/163 [==============================] - 71s 433ms/step - loss: 0.8034 - accuracy: 0.6903 - val_loss: 2.8512 - val_accuracy: 0.3447\n",
      "Epoch 245/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7864 - accuracy: 0.6862 - val_loss: 2.8031 - val_accuracy: 0.3505\n",
      "Epoch 246/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7955 - accuracy: 0.6967 - val_loss: 2.8594 - val_accuracy: 0.3496\n",
      "Epoch 247/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7812 - accuracy: 0.6990 - val_loss: 2.8558 - val_accuracy: 0.3501\n",
      "Epoch 248/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7831 - accuracy: 0.6988 - val_loss: 3.1104 - val_accuracy: 0.3510\n",
      "Epoch 249/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7669 - accuracy: 0.7030 - val_loss: 2.8969 - val_accuracy: 0.3641\n",
      "Epoch 250/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.8007 - accuracy: 0.6899 - val_loss: 2.9779 - val_accuracy: 0.3569\n",
      "Epoch 251/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7789 - accuracy: 0.7038 - val_loss: 2.9500 - val_accuracy: 0.3546\n",
      "Epoch 252/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.8099 - accuracy: 0.6822 - val_loss: 2.9488 - val_accuracy: 0.3659\n",
      "Epoch 253/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7848 - accuracy: 0.6966 - val_loss: 2.9880 - val_accuracy: 0.3560\n",
      "Epoch 254/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7886 - accuracy: 0.7055 - val_loss: 2.9756 - val_accuracy: 0.3551\n",
      "Epoch 255/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7889 - accuracy: 0.6963 - val_loss: 2.8460 - val_accuracy: 0.3528\n",
      "Epoch 256/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.8099 - accuracy: 0.6913 - val_loss: 2.8268 - val_accuracy: 0.3533\n",
      "Epoch 257/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7911 - accuracy: 0.6932 - val_loss: 2.7831 - val_accuracy: 0.3465\n",
      "Epoch 258/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7861 - accuracy: 0.6984 - val_loss: 3.0688 - val_accuracy: 0.3614\n",
      "Epoch 259/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.7724 - accuracy: 0.7036 - val_loss: 2.8629 - val_accuracy: 0.3587\n",
      "Epoch 260/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7873 - accuracy: 0.6966 - val_loss: 2.7859 - val_accuracy: 0.3551\n",
      "Epoch 261/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7742 - accuracy: 0.6959 - val_loss: 3.0886 - val_accuracy: 0.3641\n",
      "Epoch 262/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.7857 - accuracy: 0.6965 - val_loss: 2.6480 - val_accuracy: 0.3628\n",
      "Epoch 263/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7753 - accuracy: 0.7055 - val_loss: 2.8736 - val_accuracy: 0.3505\n",
      "Epoch 264/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.7683 - accuracy: 0.7001 - val_loss: 3.2709 - val_accuracy: 0.3673\n",
      "Epoch 265/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7856 - accuracy: 0.6922 - val_loss: 2.8398 - val_accuracy: 0.3587\n",
      "Epoch 266/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7851 - accuracy: 0.7022 - val_loss: 3.2327 - val_accuracy: 0.3456\n",
      "Epoch 267/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8256 - accuracy: 0.6839 - val_loss: 2.8116 - val_accuracy: 0.3569\n",
      "Epoch 268/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7767 - accuracy: 0.7003 - val_loss: 2.9790 - val_accuracy: 0.3465\n",
      "Epoch 269/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7840 - accuracy: 0.7013 - val_loss: 2.9237 - val_accuracy: 0.3601\n",
      "Epoch 270/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7719 - accuracy: 0.7013 - val_loss: 2.9845 - val_accuracy: 0.3659\n",
      "Epoch 271/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7985 - accuracy: 0.6943 - val_loss: 2.7738 - val_accuracy: 0.3478\n",
      "Epoch 272/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.8040 - accuracy: 0.6916 - val_loss: 2.7959 - val_accuracy: 0.3632\n",
      "Epoch 273/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7809 - accuracy: 0.7020 - val_loss: 2.8743 - val_accuracy: 0.3632\n",
      "Epoch 274/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7948 - accuracy: 0.6976 - val_loss: 2.8505 - val_accuracy: 0.3424\n",
      "Epoch 275/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7709 - accuracy: 0.6990 - val_loss: 2.9135 - val_accuracy: 0.3691\n",
      "Epoch 276/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7749 - accuracy: 0.7034 - val_loss: 2.9208 - val_accuracy: 0.3596\n",
      "Epoch 277/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7756 - accuracy: 0.7038 - val_loss: 2.9019 - val_accuracy: 0.3637\n",
      "Epoch 278/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7748 - accuracy: 0.7065 - val_loss: 2.8018 - val_accuracy: 0.3487\n",
      "Epoch 279/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.7838 - accuracy: 0.7028 - val_loss: 2.9775 - val_accuracy: 0.3768\n",
      "Epoch 280/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7590 - accuracy: 0.7076 - val_loss: 2.9767 - val_accuracy: 0.3614\n",
      "Epoch 281/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7773 - accuracy: 0.6966 - val_loss: 2.9131 - val_accuracy: 0.3542\n",
      "Epoch 282/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.8056 - accuracy: 0.7017 - val_loss: 2.8323 - val_accuracy: 0.3641\n",
      "Epoch 283/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7584 - accuracy: 0.7136 - val_loss: 2.8264 - val_accuracy: 0.3641\n",
      "Epoch 284/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7511 - accuracy: 0.7128 - val_loss: 3.0939 - val_accuracy: 0.3696\n",
      "Epoch 285/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7660 - accuracy: 0.7045 - val_loss: 3.1171 - val_accuracy: 0.3601\n",
      "Epoch 286/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7912 - accuracy: 0.6949 - val_loss: 3.0391 - val_accuracy: 0.3664\n",
      "Epoch 287/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.7620 - accuracy: 0.7051 - val_loss: 2.9988 - val_accuracy: 0.3596\n",
      "Epoch 288/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7603 - accuracy: 0.7009 - val_loss: 3.1094 - val_accuracy: 0.3596\n",
      "Epoch 289/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7738 - accuracy: 0.7144 - val_loss: 3.0485 - val_accuracy: 0.3564\n",
      "Epoch 290/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7483 - accuracy: 0.7148 - val_loss: 2.9960 - val_accuracy: 0.3700\n",
      "Epoch 291/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7530 - accuracy: 0.7132 - val_loss: 2.9727 - val_accuracy: 0.3664\n",
      "Epoch 292/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7477 - accuracy: 0.7117 - val_loss: 3.2447 - val_accuracy: 0.3451\n",
      "Epoch 293/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7583 - accuracy: 0.7173 - val_loss: 2.9625 - val_accuracy: 0.3578\n",
      "Epoch 294/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7519 - accuracy: 0.7132 - val_loss: 3.2217 - val_accuracy: 0.3587\n",
      "Epoch 295/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7697 - accuracy: 0.7072 - val_loss: 3.0916 - val_accuracy: 0.3578\n",
      "Epoch 296/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7372 - accuracy: 0.7190 - val_loss: 3.0561 - val_accuracy: 0.3591\n",
      "Epoch 297/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7459 - accuracy: 0.7171 - val_loss: 2.9336 - val_accuracy: 0.3496\n",
      "Epoch 298/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7647 - accuracy: 0.7163 - val_loss: 2.9003 - val_accuracy: 0.3478\n",
      "Epoch 299/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7506 - accuracy: 0.7090 - val_loss: 3.0844 - val_accuracy: 0.3641\n",
      "Epoch 300/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7243 - accuracy: 0.7263 - val_loss: 3.1495 - val_accuracy: 0.3673\n",
      "Epoch 301/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7598 - accuracy: 0.7084 - val_loss: 3.0220 - val_accuracy: 0.3542\n",
      "Epoch 302/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7539 - accuracy: 0.7190 - val_loss: 3.2676 - val_accuracy: 0.3610\n",
      "Epoch 303/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7393 - accuracy: 0.7174 - val_loss: 3.0215 - val_accuracy: 0.3564\n",
      "Epoch 304/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7325 - accuracy: 0.7242 - val_loss: 3.0981 - val_accuracy: 0.3510\n",
      "Epoch 305/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7489 - accuracy: 0.7138 - val_loss: 3.0028 - val_accuracy: 0.3564\n",
      "Epoch 306/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7541 - accuracy: 0.7169 - val_loss: 3.0426 - val_accuracy: 0.3637\n",
      "Epoch 307/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7518 - accuracy: 0.7136 - val_loss: 2.9220 - val_accuracy: 0.3632\n",
      "Epoch 308/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7704 - accuracy: 0.7132 - val_loss: 2.9121 - val_accuracy: 0.3573\n",
      "Epoch 309/1000\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.7186 - accuracy: 0.7246 - val_loss: 3.1767 - val_accuracy: 0.3591\n",
      "Epoch 310/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7712 - accuracy: 0.7096 - val_loss: 3.0840 - val_accuracy: 0.3397\n",
      "Epoch 311/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7729 - accuracy: 0.7084 - val_loss: 2.8723 - val_accuracy: 0.3664\n",
      "Epoch 312/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7337 - accuracy: 0.7240 - val_loss: 3.1087 - val_accuracy: 0.3596\n",
      "Epoch 313/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7530 - accuracy: 0.7124 - val_loss: 2.9373 - val_accuracy: 0.3582\n",
      "Epoch 314/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7250 - accuracy: 0.7219 - val_loss: 3.0775 - val_accuracy: 0.3705\n",
      "Epoch 315/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7587 - accuracy: 0.7138 - val_loss: 3.2654 - val_accuracy: 0.3428\n",
      "Epoch 316/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7463 - accuracy: 0.7217 - val_loss: 2.9959 - val_accuracy: 0.3628\n",
      "Epoch 317/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7180 - accuracy: 0.7246 - val_loss: 3.1301 - val_accuracy: 0.3573\n",
      "Epoch 318/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7652 - accuracy: 0.7067 - val_loss: 3.0402 - val_accuracy: 0.3519\n",
      "Epoch 319/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7361 - accuracy: 0.7198 - val_loss: 3.3085 - val_accuracy: 0.3637\n",
      "Epoch 320/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7517 - accuracy: 0.7178 - val_loss: 3.2389 - val_accuracy: 0.3492\n",
      "Epoch 321/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7613 - accuracy: 0.7176 - val_loss: 3.0959 - val_accuracy: 0.3501\n",
      "Epoch 322/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7265 - accuracy: 0.7180 - val_loss: 3.0794 - val_accuracy: 0.3510\n",
      "Epoch 323/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7468 - accuracy: 0.7173 - val_loss: 2.8914 - val_accuracy: 0.3451\n",
      "Epoch 324/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7373 - accuracy: 0.7121 - val_loss: 3.0442 - val_accuracy: 0.3388\n",
      "Epoch 325/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7452 - accuracy: 0.7186 - val_loss: 3.0274 - val_accuracy: 0.3524\n",
      "Epoch 326/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.7210 - accuracy: 0.7217 - val_loss: 3.0908 - val_accuracy: 0.3700\n",
      "Epoch 327/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7570 - accuracy: 0.7144 - val_loss: 3.0318 - val_accuracy: 0.3555\n",
      "Epoch 328/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7566 - accuracy: 0.7082 - val_loss: 2.8148 - val_accuracy: 0.3659\n",
      "Epoch 329/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7498 - accuracy: 0.7122 - val_loss: 3.0017 - val_accuracy: 0.3569\n",
      "Epoch 330/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.7543 - accuracy: 0.7151 - val_loss: 3.0949 - val_accuracy: 0.3510\n",
      "Epoch 331/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7435 - accuracy: 0.7236 - val_loss: 3.1515 - val_accuracy: 0.3551\n",
      "Epoch 332/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7130 - accuracy: 0.7323 - val_loss: 3.1998 - val_accuracy: 0.3560\n",
      "Epoch 333/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.7411 - accuracy: 0.7246 - val_loss: 3.2580 - val_accuracy: 0.3496\n",
      "Epoch 334/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7222 - accuracy: 0.7279 - val_loss: 3.1766 - val_accuracy: 0.3496\n",
      "Epoch 335/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7177 - accuracy: 0.7325 - val_loss: 3.0466 - val_accuracy: 0.3573\n",
      "Epoch 336/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7146 - accuracy: 0.7257 - val_loss: 3.2340 - val_accuracy: 0.3650\n",
      "Epoch 337/1000\n",
      "163/163 [==============================] - 72s 440ms/step - loss: 0.7373 - accuracy: 0.7151 - val_loss: 3.1662 - val_accuracy: 0.3610\n",
      "Epoch 338/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7708 - accuracy: 0.7128 - val_loss: 2.9015 - val_accuracy: 0.3487\n",
      "Epoch 339/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.7447 - accuracy: 0.7153 - val_loss: 3.1079 - val_accuracy: 0.3601\n",
      "Epoch 340/1000\n",
      "163/163 [==============================] - 70s 432ms/step - loss: 0.7392 - accuracy: 0.7176 - val_loss: 3.1233 - val_accuracy: 0.3610\n",
      "Epoch 341/1000\n",
      "163/163 [==============================] - 71s 434ms/step - loss: 0.7151 - accuracy: 0.7346 - val_loss: 3.3025 - val_accuracy: 0.3605\n",
      "Epoch 342/1000\n",
      "163/163 [==============================] - 70s 430ms/step - loss: 0.7196 - accuracy: 0.7269 - val_loss: 3.0656 - val_accuracy: 0.3659\n",
      "Epoch 343/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7448 - accuracy: 0.7180 - val_loss: 3.1948 - val_accuracy: 0.3582\n",
      "Epoch 344/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7356 - accuracy: 0.7227 - val_loss: 3.1007 - val_accuracy: 0.3551\n",
      "Epoch 345/1000\n",
      "163/163 [==============================] - 70s 430ms/step - loss: 0.7544 - accuracy: 0.7121 - val_loss: 3.2309 - val_accuracy: 0.3569\n",
      "Epoch 346/1000\n",
      "163/163 [==============================] - 70s 431ms/step - loss: 0.7134 - accuracy: 0.7332 - val_loss: 2.9925 - val_accuracy: 0.3551\n",
      "Epoch 347/1000\n",
      "163/163 [==============================] - 71s 436ms/step - loss: 0.7221 - accuracy: 0.7200 - val_loss: 3.3204 - val_accuracy: 0.3610\n",
      "Epoch 348/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7644 - accuracy: 0.7159 - val_loss: 3.3320 - val_accuracy: 0.3442\n",
      "Epoch 349/1000\n",
      "163/163 [==============================] - 70s 433ms/step - loss: 0.7299 - accuracy: 0.7213 - val_loss: 3.4011 - val_accuracy: 0.3551\n",
      "Epoch 350/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6949 - accuracy: 0.7398 - val_loss: 3.4297 - val_accuracy: 0.3483\n",
      "Epoch 351/1000\n",
      "163/163 [==============================] - 71s 437ms/step - loss: 0.7029 - accuracy: 0.7302 - val_loss: 3.3323 - val_accuracy: 0.3573\n",
      "Epoch 352/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7042 - accuracy: 0.7350 - val_loss: 3.2722 - val_accuracy: 0.3478\n",
      "Epoch 353/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7080 - accuracy: 0.7340 - val_loss: 3.0255 - val_accuracy: 0.3492\n",
      "Epoch 354/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7021 - accuracy: 0.7375 - val_loss: 3.1412 - val_accuracy: 0.3551\n",
      "Epoch 355/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7135 - accuracy: 0.7325 - val_loss: 3.2755 - val_accuracy: 0.3487\n",
      "Epoch 356/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7042 - accuracy: 0.7379 - val_loss: 3.0525 - val_accuracy: 0.3560\n",
      "Epoch 357/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7072 - accuracy: 0.7307 - val_loss: 3.2165 - val_accuracy: 0.3596\n",
      "Epoch 358/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7005 - accuracy: 0.7344 - val_loss: 3.1155 - val_accuracy: 0.3591\n",
      "Epoch 359/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7086 - accuracy: 0.7288 - val_loss: 3.3744 - val_accuracy: 0.3501\n",
      "Epoch 360/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7204 - accuracy: 0.7298 - val_loss: 3.4400 - val_accuracy: 0.3546\n",
      "Epoch 361/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.6973 - accuracy: 0.7329 - val_loss: 3.2467 - val_accuracy: 0.3528\n",
      "Epoch 362/1000\n",
      "163/163 [==============================] - 71s 435ms/step - loss: 0.7065 - accuracy: 0.7323 - val_loss: 3.1308 - val_accuracy: 0.3587\n",
      "Epoch 363/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7149 - accuracy: 0.7369 - val_loss: 3.2525 - val_accuracy: 0.3628\n",
      "Epoch 364/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.7092 - accuracy: 0.7284 - val_loss: 3.1196 - val_accuracy: 0.3537\n",
      "Epoch 365/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7189 - accuracy: 0.7313 - val_loss: 3.3939 - val_accuracy: 0.3524\n",
      "Epoch 366/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.7043 - accuracy: 0.7315 - val_loss: 3.2712 - val_accuracy: 0.3601\n",
      "Epoch 367/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7099 - accuracy: 0.7307 - val_loss: 3.1451 - val_accuracy: 0.3438\n",
      "Epoch 368/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7227 - accuracy: 0.7313 - val_loss: 3.2480 - val_accuracy: 0.3551\n",
      "Epoch 369/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6853 - accuracy: 0.7373 - val_loss: 3.4185 - val_accuracy: 0.3460\n",
      "Epoch 370/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7191 - accuracy: 0.7383 - val_loss: 3.1313 - val_accuracy: 0.3424\n",
      "Epoch 371/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7425 - accuracy: 0.7238 - val_loss: 3.1904 - val_accuracy: 0.3505\n",
      "Epoch 372/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7006 - accuracy: 0.7340 - val_loss: 3.2647 - val_accuracy: 0.3546\n",
      "Epoch 373/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7135 - accuracy: 0.7240 - val_loss: 3.1589 - val_accuracy: 0.3605\n",
      "Epoch 374/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7084 - accuracy: 0.7313 - val_loss: 3.3162 - val_accuracy: 0.3614\n",
      "Epoch 375/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6879 - accuracy: 0.7446 - val_loss: 3.2196 - val_accuracy: 0.3682\n",
      "Epoch 376/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6924 - accuracy: 0.7392 - val_loss: 3.4442 - val_accuracy: 0.3587\n",
      "Epoch 377/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.7061 - accuracy: 0.7390 - val_loss: 3.4822 - val_accuracy: 0.3438\n",
      "Epoch 378/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7002 - accuracy: 0.7361 - val_loss: 3.4503 - val_accuracy: 0.3569\n",
      "Epoch 379/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.7147 - accuracy: 0.7315 - val_loss: 3.2023 - val_accuracy: 0.3623\n",
      "Epoch 380/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.7459 - accuracy: 0.7230 - val_loss: 3.0798 - val_accuracy: 0.3546\n",
      "Epoch 381/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7107 - accuracy: 0.7350 - val_loss: 2.9826 - val_accuracy: 0.3483\n",
      "Epoch 382/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.6969 - accuracy: 0.7396 - val_loss: 3.2596 - val_accuracy: 0.3582\n",
      "Epoch 383/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6830 - accuracy: 0.7421 - val_loss: 3.3431 - val_accuracy: 0.3700\n",
      "Epoch 384/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7239 - accuracy: 0.7305 - val_loss: 3.1067 - val_accuracy: 0.3551\n",
      "Epoch 385/1000\n",
      "163/163 [==============================] - 77s 470ms/step - loss: 0.6831 - accuracy: 0.7433 - val_loss: 3.3147 - val_accuracy: 0.3709\n",
      "Epoch 386/1000\n",
      "163/163 [==============================] - 83s 507ms/step - loss: 0.6897 - accuracy: 0.7402 - val_loss: 3.2402 - val_accuracy: 0.3492\n",
      "Epoch 387/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.7171 - accuracy: 0.7334 - val_loss: 3.1283 - val_accuracy: 0.3474\n",
      "Epoch 388/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.7279 - accuracy: 0.7329 - val_loss: 3.0885 - val_accuracy: 0.3718\n",
      "Epoch 389/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6869 - accuracy: 0.7460 - val_loss: 3.4659 - val_accuracy: 0.3582\n",
      "Epoch 390/1000\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.7185 - accuracy: 0.7321 - val_loss: 3.1146 - val_accuracy: 0.3664\n",
      "Epoch 391/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6826 - accuracy: 0.7433 - val_loss: 3.1701 - val_accuracy: 0.3668\n",
      "Epoch 392/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.7128 - accuracy: 0.7338 - val_loss: 3.2009 - val_accuracy: 0.3555\n",
      "Epoch 393/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6962 - accuracy: 0.7269 - val_loss: 3.4060 - val_accuracy: 0.3519\n",
      "Epoch 394/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7156 - accuracy: 0.7311 - val_loss: 3.4166 - val_accuracy: 0.3682\n",
      "Epoch 395/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.7413 - accuracy: 0.7309 - val_loss: 3.1326 - val_accuracy: 0.3596\n",
      "Epoch 396/1000\n",
      "163/163 [==============================] - 71s 438ms/step - loss: 0.7215 - accuracy: 0.7356 - val_loss: 3.1810 - val_accuracy: 0.3682\n",
      "Epoch 397/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.7185 - accuracy: 0.7321 - val_loss: 2.9654 - val_accuracy: 0.3533\n",
      "Epoch 398/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6557 - accuracy: 0.7529 - val_loss: 3.3151 - val_accuracy: 0.3650\n",
      "Epoch 399/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7029 - accuracy: 0.7375 - val_loss: 3.0198 - val_accuracy: 0.3605\n",
      "Epoch 400/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.7030 - accuracy: 0.7286 - val_loss: 3.1941 - val_accuracy: 0.3641\n",
      "Epoch 401/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6882 - accuracy: 0.7415 - val_loss: 3.2106 - val_accuracy: 0.3664\n",
      "Epoch 402/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6824 - accuracy: 0.7460 - val_loss: 3.2978 - val_accuracy: 0.3537\n",
      "Epoch 403/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7234 - accuracy: 0.7367 - val_loss: 3.2294 - val_accuracy: 0.3623\n",
      "Epoch 404/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6859 - accuracy: 0.7458 - val_loss: 3.2748 - val_accuracy: 0.3501\n",
      "Epoch 405/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.7094 - accuracy: 0.7350 - val_loss: 3.2310 - val_accuracy: 0.3628\n",
      "Epoch 406/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6788 - accuracy: 0.7496 - val_loss: 3.2483 - val_accuracy: 0.3533\n",
      "Epoch 407/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.7136 - accuracy: 0.7331 - val_loss: 3.2603 - val_accuracy: 0.3601\n",
      "Epoch 408/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6981 - accuracy: 0.7415 - val_loss: 3.3740 - val_accuracy: 0.3601\n",
      "Epoch 409/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6692 - accuracy: 0.7473 - val_loss: 3.3364 - val_accuracy: 0.3591\n",
      "Epoch 410/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.6814 - accuracy: 0.7444 - val_loss: 3.2757 - val_accuracy: 0.3591\n",
      "Epoch 411/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.6832 - accuracy: 0.7427 - val_loss: 3.2238 - val_accuracy: 0.3560\n",
      "Epoch 412/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6600 - accuracy: 0.7527 - val_loss: 3.4050 - val_accuracy: 0.3700\n",
      "Epoch 413/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6922 - accuracy: 0.7411 - val_loss: 3.2045 - val_accuracy: 0.3614\n",
      "Epoch 414/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6915 - accuracy: 0.7490 - val_loss: 3.0350 - val_accuracy: 0.3587\n",
      "Epoch 415/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6954 - accuracy: 0.7332 - val_loss: 3.3820 - val_accuracy: 0.3514\n",
      "Epoch 416/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6804 - accuracy: 0.7413 - val_loss: 3.4753 - val_accuracy: 0.3514\n",
      "Epoch 417/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6738 - accuracy: 0.7415 - val_loss: 3.5053 - val_accuracy: 0.3487\n",
      "Epoch 418/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.6872 - accuracy: 0.7448 - val_loss: 3.4406 - val_accuracy: 0.3641\n",
      "Epoch 419/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6986 - accuracy: 0.7369 - val_loss: 3.2316 - val_accuracy: 0.3401\n",
      "Epoch 420/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.6733 - accuracy: 0.7525 - val_loss: 3.2680 - val_accuracy: 0.3555\n",
      "Epoch 421/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6891 - accuracy: 0.7450 - val_loss: 3.3080 - val_accuracy: 0.3596\n",
      "Epoch 422/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6986 - accuracy: 0.7369 - val_loss: 3.3378 - val_accuracy: 0.3587\n",
      "Epoch 423/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6706 - accuracy: 0.7481 - val_loss: 3.3879 - val_accuracy: 0.3564\n",
      "Epoch 424/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6951 - accuracy: 0.7454 - val_loss: 3.1466 - val_accuracy: 0.3573\n",
      "Epoch 425/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.7063 - accuracy: 0.7375 - val_loss: 3.2362 - val_accuracy: 0.3505\n",
      "Epoch 426/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6824 - accuracy: 0.7440 - val_loss: 3.3527 - val_accuracy: 0.3551\n",
      "Epoch 427/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6733 - accuracy: 0.7504 - val_loss: 3.2019 - val_accuracy: 0.3419\n",
      "Epoch 428/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6847 - accuracy: 0.7433 - val_loss: 3.3386 - val_accuracy: 0.3632\n",
      "Epoch 429/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6809 - accuracy: 0.7411 - val_loss: 3.2227 - val_accuracy: 0.3632\n",
      "Epoch 430/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7041 - accuracy: 0.7409 - val_loss: 3.1469 - val_accuracy: 0.3560\n",
      "Epoch 431/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.6778 - accuracy: 0.7415 - val_loss: 3.3769 - val_accuracy: 0.3524\n",
      "Epoch 432/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6773 - accuracy: 0.7494 - val_loss: 3.2708 - val_accuracy: 0.3528\n",
      "Epoch 433/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6692 - accuracy: 0.7533 - val_loss: 3.3087 - val_accuracy: 0.3655\n",
      "Epoch 434/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6530 - accuracy: 0.7575 - val_loss: 3.3574 - val_accuracy: 0.3505\n",
      "Epoch 435/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6725 - accuracy: 0.7469 - val_loss: 3.2212 - val_accuracy: 0.3478\n",
      "Epoch 436/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.7157 - accuracy: 0.7323 - val_loss: 3.3306 - val_accuracy: 0.3537\n",
      "Epoch 437/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7062 - accuracy: 0.7394 - val_loss: 2.9990 - val_accuracy: 0.3419\n",
      "Epoch 438/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.6865 - accuracy: 0.7458 - val_loss: 3.2815 - val_accuracy: 0.3474\n",
      "Epoch 439/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.6578 - accuracy: 0.7498 - val_loss: 3.2340 - val_accuracy: 0.3533\n",
      "Epoch 440/1000\n",
      "163/163 [==============================] - 80s 493ms/step - loss: 0.6778 - accuracy: 0.7461 - val_loss: 3.3301 - val_accuracy: 0.3478\n",
      "Epoch 441/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7126 - accuracy: 0.7415 - val_loss: 3.3067 - val_accuracy: 0.3542\n",
      "Epoch 442/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.6734 - accuracy: 0.7494 - val_loss: 3.2747 - val_accuracy: 0.3524\n",
      "Epoch 443/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6941 - accuracy: 0.7421 - val_loss: 3.1919 - val_accuracy: 0.3596\n",
      "Epoch 444/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6810 - accuracy: 0.7506 - val_loss: 3.2524 - val_accuracy: 0.3551\n",
      "Epoch 445/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6961 - accuracy: 0.7388 - val_loss: 3.0790 - val_accuracy: 0.3492\n",
      "Epoch 446/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6988 - accuracy: 0.7404 - val_loss: 3.3218 - val_accuracy: 0.3501\n",
      "Epoch 447/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6574 - accuracy: 0.7485 - val_loss: 3.4232 - val_accuracy: 0.3528\n",
      "Epoch 448/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6622 - accuracy: 0.7539 - val_loss: 3.5134 - val_accuracy: 0.3510\n",
      "Epoch 449/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6988 - accuracy: 0.7398 - val_loss: 3.2311 - val_accuracy: 0.3460\n",
      "Epoch 450/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.7033 - accuracy: 0.7421 - val_loss: 3.1511 - val_accuracy: 0.3505\n",
      "Epoch 451/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6795 - accuracy: 0.7427 - val_loss: 3.4558 - val_accuracy: 0.3433\n",
      "Epoch 452/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6880 - accuracy: 0.7446 - val_loss: 3.2659 - val_accuracy: 0.3428\n",
      "Epoch 453/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.7158 - accuracy: 0.7367 - val_loss: 3.0616 - val_accuracy: 0.3551\n",
      "Epoch 454/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6740 - accuracy: 0.7506 - val_loss: 3.3354 - val_accuracy: 0.3279\n",
      "Epoch 455/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.7143 - accuracy: 0.7369 - val_loss: 3.1628 - val_accuracy: 0.3505\n",
      "Epoch 456/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.7160 - accuracy: 0.7390 - val_loss: 3.1652 - val_accuracy: 0.3469\n",
      "Epoch 457/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6880 - accuracy: 0.7465 - val_loss: 3.1874 - val_accuracy: 0.3551\n",
      "Epoch 458/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.7186 - accuracy: 0.7361 - val_loss: 3.0140 - val_accuracy: 0.3564\n",
      "Epoch 459/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6776 - accuracy: 0.7504 - val_loss: 3.1939 - val_accuracy: 0.3641\n",
      "Epoch 460/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6966 - accuracy: 0.7357 - val_loss: 3.4319 - val_accuracy: 0.3528\n",
      "Epoch 461/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6980 - accuracy: 0.7392 - val_loss: 3.2574 - val_accuracy: 0.3542\n",
      "Epoch 462/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6619 - accuracy: 0.7562 - val_loss: 3.2377 - val_accuracy: 0.3551\n",
      "Epoch 463/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6582 - accuracy: 0.7533 - val_loss: 3.4083 - val_accuracy: 0.3673\n",
      "Epoch 464/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6895 - accuracy: 0.7440 - val_loss: 3.2036 - val_accuracy: 0.3474\n",
      "Epoch 465/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6775 - accuracy: 0.7483 - val_loss: 3.4090 - val_accuracy: 0.3465\n",
      "Epoch 466/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6829 - accuracy: 0.7452 - val_loss: 3.4804 - val_accuracy: 0.3492\n",
      "Epoch 467/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6954 - accuracy: 0.7394 - val_loss: 3.3254 - val_accuracy: 0.3465\n",
      "Epoch 468/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6670 - accuracy: 0.7535 - val_loss: 3.4019 - val_accuracy: 0.3546\n",
      "Epoch 469/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6814 - accuracy: 0.7525 - val_loss: 3.2054 - val_accuracy: 0.3605\n",
      "Epoch 470/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6669 - accuracy: 0.7604 - val_loss: 3.3356 - val_accuracy: 0.3555\n",
      "Epoch 471/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6866 - accuracy: 0.7415 - val_loss: 3.3287 - val_accuracy: 0.3528\n",
      "Epoch 472/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6414 - accuracy: 0.7641 - val_loss: 3.3255 - val_accuracy: 0.3573\n",
      "Epoch 473/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6648 - accuracy: 0.7546 - val_loss: 3.4923 - val_accuracy: 0.3456\n",
      "Epoch 474/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6671 - accuracy: 0.7529 - val_loss: 3.3923 - val_accuracy: 0.3628\n",
      "Epoch 475/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6680 - accuracy: 0.7506 - val_loss: 3.3267 - val_accuracy: 0.3533\n",
      "Epoch 476/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6602 - accuracy: 0.7587 - val_loss: 3.1461 - val_accuracy: 0.3514\n",
      "Epoch 477/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6690 - accuracy: 0.7525 - val_loss: 3.3951 - val_accuracy: 0.3555\n",
      "Epoch 478/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6456 - accuracy: 0.7612 - val_loss: 3.2492 - val_accuracy: 0.3623\n",
      "Epoch 479/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6456 - accuracy: 0.7635 - val_loss: 3.4916 - val_accuracy: 0.3474\n",
      "Epoch 480/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6871 - accuracy: 0.7444 - val_loss: 3.3487 - val_accuracy: 0.3542\n",
      "Epoch 481/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6636 - accuracy: 0.7523 - val_loss: 3.4984 - val_accuracy: 0.3573\n",
      "Epoch 482/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6609 - accuracy: 0.7469 - val_loss: 3.4938 - val_accuracy: 0.3492\n",
      "Epoch 483/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6544 - accuracy: 0.7610 - val_loss: 3.3939 - val_accuracy: 0.3528\n",
      "Epoch 484/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6243 - accuracy: 0.7698 - val_loss: 3.5012 - val_accuracy: 0.3474\n",
      "Epoch 485/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6538 - accuracy: 0.7598 - val_loss: 3.4522 - val_accuracy: 0.3519\n",
      "Epoch 486/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6584 - accuracy: 0.7556 - val_loss: 3.4989 - val_accuracy: 0.3469\n",
      "Epoch 487/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6676 - accuracy: 0.7539 - val_loss: 3.2975 - val_accuracy: 0.3537\n",
      "Epoch 488/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6610 - accuracy: 0.7564 - val_loss: 3.6778 - val_accuracy: 0.3492\n",
      "Epoch 489/1000\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.6652 - accuracy: 0.7502 - val_loss: 3.2820 - val_accuracy: 0.3542\n",
      "Epoch 490/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6566 - accuracy: 0.7608 - val_loss: 3.2897 - val_accuracy: 0.3478\n",
      "Epoch 491/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6623 - accuracy: 0.7550 - val_loss: 3.5203 - val_accuracy: 0.3447\n",
      "Epoch 492/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6555 - accuracy: 0.7610 - val_loss: 3.5408 - val_accuracy: 0.3433\n",
      "Epoch 493/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6583 - accuracy: 0.7591 - val_loss: 3.2715 - val_accuracy: 0.3573\n",
      "Epoch 494/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6810 - accuracy: 0.7573 - val_loss: 3.1635 - val_accuracy: 0.3623\n",
      "Epoch 495/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6740 - accuracy: 0.7452 - val_loss: 3.6370 - val_accuracy: 0.3501\n",
      "Epoch 496/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6563 - accuracy: 0.7583 - val_loss: 3.3895 - val_accuracy: 0.3528\n",
      "Epoch 497/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6430 - accuracy: 0.7641 - val_loss: 3.4378 - val_accuracy: 0.3474\n",
      "Epoch 498/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6563 - accuracy: 0.7577 - val_loss: 3.3865 - val_accuracy: 0.3510\n",
      "Epoch 499/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6530 - accuracy: 0.7562 - val_loss: 3.3184 - val_accuracy: 0.3415\n",
      "Epoch 500/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6814 - accuracy: 0.7500 - val_loss: 3.3866 - val_accuracy: 0.3483\n",
      "Epoch 501/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6738 - accuracy: 0.7448 - val_loss: 3.4530 - val_accuracy: 0.3474\n",
      "Epoch 502/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6305 - accuracy: 0.7710 - val_loss: 3.4177 - val_accuracy: 0.3578\n",
      "Epoch 503/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6481 - accuracy: 0.7598 - val_loss: 3.4143 - val_accuracy: 0.3628\n",
      "Epoch 504/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6531 - accuracy: 0.7610 - val_loss: 3.6139 - val_accuracy: 0.3496\n",
      "Epoch 505/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.6510 - accuracy: 0.7596 - val_loss: 3.2937 - val_accuracy: 0.3478\n",
      "Epoch 506/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6630 - accuracy: 0.7539 - val_loss: 3.5243 - val_accuracy: 0.3546\n",
      "Epoch 507/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6550 - accuracy: 0.7556 - val_loss: 3.3271 - val_accuracy: 0.3365\n",
      "Epoch 508/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6419 - accuracy: 0.7631 - val_loss: 3.3700 - val_accuracy: 0.3537\n",
      "Epoch 509/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6428 - accuracy: 0.7681 - val_loss: 3.2465 - val_accuracy: 0.3596\n",
      "Epoch 510/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6748 - accuracy: 0.7521 - val_loss: 3.2738 - val_accuracy: 0.3510\n",
      "Epoch 511/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6277 - accuracy: 0.7693 - val_loss: 3.5362 - val_accuracy: 0.3650\n",
      "Epoch 512/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6664 - accuracy: 0.7546 - val_loss: 3.4909 - val_accuracy: 0.3351\n",
      "Epoch 513/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6678 - accuracy: 0.7552 - val_loss: 3.3722 - val_accuracy: 0.3433\n",
      "Epoch 514/1000\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.6614 - accuracy: 0.7583 - val_loss: 3.3996 - val_accuracy: 0.3528\n",
      "Epoch 515/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6641 - accuracy: 0.7560 - val_loss: 3.3348 - val_accuracy: 0.3533\n",
      "Epoch 516/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6275 - accuracy: 0.7700 - val_loss: 3.5567 - val_accuracy: 0.3623\n",
      "Epoch 517/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6350 - accuracy: 0.7668 - val_loss: 3.5205 - val_accuracy: 0.3623\n",
      "Epoch 518/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6419 - accuracy: 0.7650 - val_loss: 3.5139 - val_accuracy: 0.3578\n",
      "Epoch 519/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6559 - accuracy: 0.7592 - val_loss: 3.4517 - val_accuracy: 0.3433\n",
      "Epoch 520/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6322 - accuracy: 0.7706 - val_loss: 3.6924 - val_accuracy: 0.3524\n",
      "Epoch 521/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6639 - accuracy: 0.7556 - val_loss: 3.4570 - val_accuracy: 0.3460\n",
      "Epoch 522/1000\n",
      "163/163 [==============================] - 77s 473ms/step - loss: 0.6444 - accuracy: 0.7677 - val_loss: 3.4698 - val_accuracy: 0.3474\n",
      "Epoch 523/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6311 - accuracy: 0.7664 - val_loss: 3.7044 - val_accuracy: 0.3524\n",
      "Epoch 524/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6342 - accuracy: 0.7712 - val_loss: 3.4682 - val_accuracy: 0.3460\n",
      "Epoch 525/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6489 - accuracy: 0.7602 - val_loss: 3.6016 - val_accuracy: 0.3438\n",
      "Epoch 526/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6433 - accuracy: 0.7637 - val_loss: 3.3922 - val_accuracy: 0.3392\n",
      "Epoch 527/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.6467 - accuracy: 0.7606 - val_loss: 3.3946 - val_accuracy: 0.3659\n",
      "Epoch 528/1000\n",
      "163/163 [==============================] - 72s 442ms/step - loss: 0.6540 - accuracy: 0.7616 - val_loss: 3.5177 - val_accuracy: 0.3668\n",
      "Epoch 529/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6447 - accuracy: 0.7689 - val_loss: 3.3226 - val_accuracy: 0.3573\n",
      "Epoch 530/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6276 - accuracy: 0.7621 - val_loss: 3.3905 - val_accuracy: 0.3673\n",
      "Epoch 531/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6306 - accuracy: 0.7679 - val_loss: 3.6604 - val_accuracy: 0.3442\n",
      "Epoch 532/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6385 - accuracy: 0.7629 - val_loss: 3.3461 - val_accuracy: 0.3605\n",
      "Epoch 533/1000\n",
      "163/163 [==============================] - 73s 445ms/step - loss: 0.6193 - accuracy: 0.7702 - val_loss: 3.6904 - val_accuracy: 0.3637\n",
      "Epoch 534/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6378 - accuracy: 0.7598 - val_loss: 3.5144 - val_accuracy: 0.3646\n",
      "Epoch 535/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.6465 - accuracy: 0.7596 - val_loss: 3.5342 - val_accuracy: 0.3555\n",
      "Epoch 536/1000\n",
      "163/163 [==============================] - 73s 448ms/step - loss: 0.6393 - accuracy: 0.7631 - val_loss: 3.4294 - val_accuracy: 0.3569\n",
      "Epoch 537/1000\n",
      "163/163 [==============================] - 72s 439ms/step - loss: 0.6360 - accuracy: 0.7654 - val_loss: 3.7002 - val_accuracy: 0.3514\n",
      "Epoch 538/1000\n",
      "163/163 [==============================] - 73s 447ms/step - loss: 0.6294 - accuracy: 0.7673 - val_loss: 3.6363 - val_accuracy: 0.3465\n",
      "Epoch 539/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6612 - accuracy: 0.7577 - val_loss: 3.3320 - val_accuracy: 0.3560\n",
      "Epoch 540/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.6709 - accuracy: 0.7596 - val_loss: 3.4241 - val_accuracy: 0.3410\n",
      "Epoch 541/1000\n",
      "163/163 [==============================] - 73s 453ms/step - loss: 0.6299 - accuracy: 0.7756 - val_loss: 3.6296 - val_accuracy: 0.3628\n",
      "Epoch 542/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6559 - accuracy: 0.7610 - val_loss: 3.5540 - val_accuracy: 0.3537\n",
      "Epoch 543/1000\n",
      "163/163 [==============================] - 72s 445ms/step - loss: 0.6403 - accuracy: 0.7646 - val_loss: 3.4132 - val_accuracy: 0.3533\n",
      "Epoch 544/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6298 - accuracy: 0.7637 - val_loss: 3.3483 - val_accuracy: 0.3533\n",
      "Epoch 545/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6358 - accuracy: 0.7687 - val_loss: 3.2286 - val_accuracy: 0.3496\n",
      "Epoch 546/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6382 - accuracy: 0.7648 - val_loss: 3.3984 - val_accuracy: 0.3505\n",
      "Epoch 547/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6315 - accuracy: 0.7631 - val_loss: 3.3718 - val_accuracy: 0.3460\n",
      "Epoch 548/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6542 - accuracy: 0.7596 - val_loss: 3.5070 - val_accuracy: 0.3533\n",
      "Epoch 549/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6408 - accuracy: 0.7635 - val_loss: 3.4984 - val_accuracy: 0.3496\n",
      "Epoch 550/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6238 - accuracy: 0.7721 - val_loss: 3.3748 - val_accuracy: 0.3492\n",
      "Epoch 551/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6565 - accuracy: 0.7519 - val_loss: 3.2361 - val_accuracy: 0.3401\n",
      "Epoch 552/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6321 - accuracy: 0.7691 - val_loss: 3.3518 - val_accuracy: 0.3551\n",
      "Epoch 553/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6252 - accuracy: 0.7673 - val_loss: 3.4119 - val_accuracy: 0.3605\n",
      "Epoch 554/1000\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.6246 - accuracy: 0.7641 - val_loss: 3.3034 - val_accuracy: 0.3528\n",
      "Epoch 555/1000\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.6539 - accuracy: 0.7573 - val_loss: 3.4603 - val_accuracy: 0.3442\n",
      "Epoch 556/1000\n",
      "163/163 [==============================] - 74s 451ms/step - loss: 0.6228 - accuracy: 0.7764 - val_loss: 3.4547 - val_accuracy: 0.3460\n",
      "Epoch 557/1000\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.6251 - accuracy: 0.7721 - val_loss: 3.4270 - val_accuracy: 0.3460\n",
      "Epoch 558/1000\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.6519 - accuracy: 0.7585 - val_loss: 3.4344 - val_accuracy: 0.3474\n",
      "Epoch 559/1000\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.6266 - accuracy: 0.7681 - val_loss: 3.3582 - val_accuracy: 0.3623\n",
      "Epoch 560/1000\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.6371 - accuracy: 0.7704 - val_loss: 3.2807 - val_accuracy: 0.3496\n",
      "Epoch 561/1000\n",
      "163/163 [==============================] - 75s 462ms/step - loss: 0.6430 - accuracy: 0.7689 - val_loss: 3.4601 - val_accuracy: 0.3510\n",
      "Epoch 562/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6277 - accuracy: 0.7700 - val_loss: 3.2332 - val_accuracy: 0.3374\n",
      "Epoch 563/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6100 - accuracy: 0.7766 - val_loss: 3.6428 - val_accuracy: 0.3714\n",
      "Epoch 564/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6059 - accuracy: 0.7787 - val_loss: 3.5020 - val_accuracy: 0.3578\n",
      "Epoch 565/1000\n",
      "163/163 [==============================] - 75s 459ms/step - loss: 0.6336 - accuracy: 0.7629 - val_loss: 3.6283 - val_accuracy: 0.3745\n",
      "Epoch 566/1000\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.6067 - accuracy: 0.7741 - val_loss: 3.4224 - val_accuracy: 0.3465\n",
      "Epoch 567/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6293 - accuracy: 0.7637 - val_loss: 3.4293 - val_accuracy: 0.3587\n",
      "Epoch 568/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6256 - accuracy: 0.7673 - val_loss: 3.6240 - val_accuracy: 0.3483\n",
      "Epoch 569/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6556 - accuracy: 0.7614 - val_loss: 3.3628 - val_accuracy: 0.3560\n",
      "Epoch 570/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6501 - accuracy: 0.7648 - val_loss: 3.3061 - val_accuracy: 0.3732\n",
      "Epoch 571/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6159 - accuracy: 0.7689 - val_loss: 3.3668 - val_accuracy: 0.3601\n",
      "Epoch 572/1000\n",
      "163/163 [==============================] - 74s 454ms/step - loss: 0.6416 - accuracy: 0.7646 - val_loss: 3.5255 - val_accuracy: 0.3687\n",
      "Epoch 573/1000\n",
      "163/163 [==============================] - 76s 464ms/step - loss: 0.6353 - accuracy: 0.7752 - val_loss: 3.4521 - val_accuracy: 0.3505\n",
      "Epoch 574/1000\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.6495 - accuracy: 0.7723 - val_loss: 3.2768 - val_accuracy: 0.3573\n",
      "Epoch 575/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.5948 - accuracy: 0.7829 - val_loss: 3.4928 - val_accuracy: 0.3709\n",
      "Epoch 576/1000\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.6497 - accuracy: 0.7639 - val_loss: 3.2894 - val_accuracy: 0.3505\n",
      "Epoch 577/1000\n",
      "163/163 [==============================] - 74s 452ms/step - loss: 0.6416 - accuracy: 0.7664 - val_loss: 3.4681 - val_accuracy: 0.3510\n",
      "Epoch 578/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6129 - accuracy: 0.7741 - val_loss: 3.3731 - val_accuracy: 0.3528\n",
      "Epoch 579/1000\n",
      "163/163 [==============================] - 74s 455ms/step - loss: 0.6105 - accuracy: 0.7764 - val_loss: 3.6049 - val_accuracy: 0.3601\n",
      "Epoch 580/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6127 - accuracy: 0.7762 - val_loss: 3.9158 - val_accuracy: 0.3487\n",
      "Epoch 581/1000\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.6251 - accuracy: 0.7666 - val_loss: 3.6369 - val_accuracy: 0.3478\n",
      "Epoch 582/1000\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.6244 - accuracy: 0.7754 - val_loss: 3.6231 - val_accuracy: 0.3646\n",
      "Epoch 583/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6359 - accuracy: 0.7625 - val_loss: 3.4160 - val_accuracy: 0.3510\n",
      "Epoch 584/1000\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.6205 - accuracy: 0.7733 - val_loss: 3.5783 - val_accuracy: 0.3705\n",
      "Epoch 585/1000\n",
      "163/163 [==============================] - 75s 463ms/step - loss: 0.6309 - accuracy: 0.7625 - val_loss: 3.4015 - val_accuracy: 0.3591\n",
      "Epoch 586/1000\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.6377 - accuracy: 0.7616 - val_loss: 3.6199 - val_accuracy: 0.3687\n",
      "Epoch 587/1000\n",
      "163/163 [==============================] - 76s 467ms/step - loss: 0.6360 - accuracy: 0.7689 - val_loss: 3.4392 - val_accuracy: 0.3582\n",
      "Epoch 588/1000\n",
      "163/163 [==============================] - 72s 441ms/step - loss: 0.6267 - accuracy: 0.7706 - val_loss: 3.4950 - val_accuracy: 0.3632\n",
      "Epoch 589/1000\n",
      "163/163 [==============================] - 70s 428ms/step - loss: 0.6216 - accuracy: 0.7721 - val_loss: 3.2903 - val_accuracy: 0.3505\n",
      "Epoch 590/1000\n",
      "163/163 [==============================] - 75s 463ms/step - loss: 0.6396 - accuracy: 0.7646 - val_loss: 3.5216 - val_accuracy: 0.3659\n",
      "Epoch 591/1000\n",
      "163/163 [==============================] - 76s 469ms/step - loss: 0.6216 - accuracy: 0.7710 - val_loss: 3.5984 - val_accuracy: 0.3678\n",
      "Epoch 592/1000\n",
      "163/163 [==============================] - 76s 464ms/step - loss: 0.6180 - accuracy: 0.7747 - val_loss: 3.7058 - val_accuracy: 0.3564\n",
      "Epoch 593/1000\n",
      "163/163 [==============================] - 78s 475ms/step - loss: 0.6399 - accuracy: 0.7689 - val_loss: 3.7978 - val_accuracy: 0.3578\n",
      "Epoch 594/1000\n",
      "163/163 [==============================] - 80s 494ms/step - loss: 0.6074 - accuracy: 0.7783 - val_loss: 3.6339 - val_accuracy: 0.3551\n",
      "Epoch 595/1000\n",
      "163/163 [==============================] - 73s 449ms/step - loss: 0.6074 - accuracy: 0.7739 - val_loss: 3.5846 - val_accuracy: 0.3601\n",
      "Epoch 596/1000\n",
      "163/163 [==============================] - 80s 491ms/step - loss: 0.5987 - accuracy: 0.7858 - val_loss: 3.7953 - val_accuracy: 0.3505\n",
      "Epoch 597/1000\n",
      "163/163 [==============================] - 82s 502ms/step - loss: 0.6429 - accuracy: 0.7654 - val_loss: 3.2890 - val_accuracy: 0.3447\n",
      "Epoch 598/1000\n",
      "163/163 [==============================] - 83s 511ms/step - loss: 0.6225 - accuracy: 0.7718 - val_loss: 3.3584 - val_accuracy: 0.3537\n",
      "Epoch 599/1000\n",
      "163/163 [==============================] - 81s 496ms/step - loss: 0.6406 - accuracy: 0.7641 - val_loss: 3.6224 - val_accuracy: 0.3591\n",
      "Epoch 600/1000\n",
      "163/163 [==============================] - 72s 444ms/step - loss: 0.6310 - accuracy: 0.7648 - val_loss: 3.5554 - val_accuracy: 0.3632\n",
      "Epoch 601/1000\n",
      "163/163 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.7643"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m step_size_train \u001b[38;5;241m=\u001b[39m train_gen\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_gen\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m      3\u001b[0m step_size_val \u001b[38;5;241m=\u001b[39m val_gen\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mval_gen\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m----> 5\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:2209\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2198\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2199\u001b[0m \n\u001b[1;32m   2200\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m  this endpoint.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2208\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 2209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1408\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1409\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1419\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1716\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1715\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1716\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1718\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "step_size_train = train_gen.n//train_gen.batch_size\n",
    "step_size_val = val_gen.n//val_gen.batch_size\n",
    "\n",
    "h = model.fit_generator(generator = train_gen,\n",
    "                    steps_per_epoch=step_size_train,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=step_size_val,\n",
    "                    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1891f-62d3-4905-a306-c2a4818d2ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
